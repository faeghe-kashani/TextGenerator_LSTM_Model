{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "mEMBuNPOqoSx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Stateless_RNN:\n",
        "    def __init__(self,n_steps = 200):\n",
        "        self.n_steps = n_steps\n",
        "        self.model = None\n",
        "        self.dataset = None\n",
        "        self.max_id = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "    def prepare_data(self,path, batch_size = 128):\n",
        "        with open(path ,encoding=\"utf8\") as f:\n",
        "            text=f.read()\n",
        "\n",
        "        self.tokenizer = keras.preprocessing.text.Tokenizer(char_level = True)\n",
        "        self.tokenizer.fit_on_texts([text])\n",
        "        self.max_id = len(self.tokenizer.word_index)\n",
        "\n",
        "        [encoded] = np.array(self.tokenizer.texts_to_sequences([text])) -1\n",
        "        train_size = len(encoded) * 90//100\n",
        "\n",
        "        self.dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "\n",
        "\n",
        "        window_length = self.n_steps + 1\n",
        "        self.dataset = self.dataset.window(window_length , shift =1 ,drop_remainder = True)\n",
        "        self.dataset = self.dataset.flat_map(lambda window : window.batch(window_length))\n",
        "\n",
        "\n",
        "        self.dataset = self.dataset.shuffle(10000).batch(batch_size)\n",
        "        self.dataset = self.dataset.map(lambda windows:(windows[:,:-1] , windows[:,1:]))\n",
        "        self.dataset = self.dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=self.max_id),Y_batch))\n",
        "        self.dataset = self.dataset.prefetch(1)\n",
        "\n",
        "    def build_layers(self):\n",
        "        self.model = keras.models.Sequential([\n",
        "        keras.layers.LSTM(128, return_sequences=True, input_shape=[None,self.max_id], dropout=0.2, recurrent_dropout=0.2),\n",
        "        keras.layers.LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
        "        keras.layers.TimeDistributed(keras.layers.Dense(self.max_id,activation=\"softmax\"))])\n",
        "\n",
        "        return self.model.summary()\n",
        "\n",
        "    def train(self,epochs=5):\n",
        "        self.model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "        self.model.fit(self.dataset, batch_size=128, epochs=epochs)\n",
        "\n",
        "    def save_model(self,path):\n",
        "        self.model.save(path)\n",
        "\n",
        "    def load_model(self,path):\n",
        "        self.model = keras.models.load_model(path)\n",
        "\n",
        "\n",
        "    def preprocess_input(self,text):\n",
        "        X = np.array(self.tokenizer.texts_to_sequences(text)) -1\n",
        "        return tf.one_hot(X,depth =self.max_id)\n",
        "\n",
        "    def predict_next_char(self,text,temperature):\n",
        "        X_new = self.preprocess_input([text])\n",
        "\n",
        "        y_proba = self.model.predict(X_new, verbose=0)[0, -1:, :]\n",
        "\n",
        "        rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "\n",
        "        char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
        "        return self.tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
        "\n",
        "    def complete_text(self,text, n_chars=200, temperature=1):\n",
        "        for _ in range(n_chars):\n",
        "            text += self.predict_next_char(text, temperature)\n",
        "        return text"
      ],
      "metadata": {
        "id": "vvUccW6oqv4O"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResetStatesCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs):\n",
        "        self.model.reset_states()"
      ],
      "metadata": {
        "id": "VZ4xz0rwqyjn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Statefull_RNN:\n",
        "    def __init__(self,n_steps = 200):\n",
        "        self.n_steps = n_steps\n",
        "        self.batch_size = None\n",
        "        self.model = None\n",
        "        self.dataset = None\n",
        "        self.max_id = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "    def prepare_data(self,path, batch_size = 1):\n",
        "        self.batch_size = batch_size\n",
        "        with open(path ,encoding=\"utf8\") as f:\n",
        "            text=f.read()\n",
        "\n",
        "        self.tokenizer = keras.preprocessing.text.Tokenizer(char_level = True)\n",
        "        self.tokenizer.fit_on_texts([text])\n",
        "        self.max_id = len(self.tokenizer.word_index)\n",
        "\n",
        "        [encoded] = np.array(self.tokenizer.texts_to_sequences([text])) -1\n",
        "        train_size = len(encoded) * 90//100\n",
        "\n",
        "\n",
        "        window_length = self.n_steps + 1\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "        dataset = dataset.window(window_length, shift=self.n_steps,\n",
        "        drop_remainder=True)\n",
        "        dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "        dataset = dataset.batch(1)\n",
        "        dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "        dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=self.max_id), Y_batch))\n",
        "        dataset= dataset.prefetch(1)\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def build_layers(self):\n",
        "        self.model = keras.models.Sequential([keras.layers.LSTM(128, return_sequences=True, stateful=True,\n",
        "        dropout=0.2, recurrent_dropout=0.2,batch_input_shape=[self.batch_size, None, self.max_id]),\n",
        "        keras.layers.LSTM(128, return_sequences=True, stateful=True, dropout=0.2, recurrent_dropout=0.2),\n",
        "        keras.layers.TimeDistributed(keras.layers.Dense(self.max_id, activation=\"softmax\"))\n",
        "        ])\n",
        "\n",
        "    def train(self,epochs=5):\n",
        "        self.model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "        self.model.fit(self.dataset, epochs=20, callbacks=[ResetStatesCallback()])\n",
        "\n",
        "\n",
        "    def save_model(self,path):\n",
        "        self.model.save(path)\n",
        "\n",
        "    def load_model(self,path):\n",
        "        self.model = keras.models.load_model(path)\n",
        "\n",
        "\n",
        "    def preprocess_input(self,text):\n",
        "        X = np.array(self.tokenizer.texts_to_sequences(text)) -1\n",
        "        return tf.one_hot(X,depth =self.max_id)\n",
        "\n",
        "    def predict_next_char(self,text,temperature):\n",
        "        X_new = self.preprocess_input([text])\n",
        "\n",
        "        y_proba = self.model.predict(X_new)[0, -1:, :]\n",
        "\n",
        "        rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "\n",
        "        char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
        "        return self.tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
        "\n",
        "    def complete_text(self,text, n_chars=200, temperature=1):\n",
        "        for _ in range(n_chars):\n",
        "            text += self.predict_next_char(text, temperature)\n",
        "        return text"
      ],
      "metadata": {
        "id": "pCjOqNNsq46T"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Stateless_RNN()\n",
        "model.prepare_data('hafez.txt')\n",
        "model.build_layers()"
      ],
      "metadata": {
        "id": "MosMxPh9q7vn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c948a61-992f-45dd-a5b7-b5e34f6253f3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_4 (LSTM)               (None, None, 128)         91136     \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, None, 128)         131584    \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDi  (None, None, 49)          6321      \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 229041 (894.69 KB)\n",
            "Trainable params: 229041 (894.69 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train(epochs=20)"
      ],
      "metadata": {
        "id": "34P_KaWlq-ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.complete_text(text ='ساقی',n_chars=100))"
      ],
      "metadata": {
        "id": "SE-cA6UerC4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ساقی چشمت بين و من جان بينم\n",
        "\n",
        "به صوت ملک هلای توزو می‌بينم\n",
        "\n",
        "لب در سر حسن و روزی تو شد از هم\n",
        "\n",
        "گر نتورم دری"
      ],
      "metadata": {
        "id": "qiDt4IowT59i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.complete_text(text ='من',n_chars=100))"
      ],
      "metadata": {
        "id": "lcF_MgqOrFQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "منزل ريا که روز و بيشت در زلف تو در سرم\n",
        "\n",
        "گر به آب جورش علم گشا از پادشهان\n",
        "\n",
        "چند نظمی آگه می‌زنمت خبر درد"
      ],
      "metadata": {
        "id": "kgOI9QluUMKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.complete_text(text ='خانه',n_chars=100))"
      ],
      "metadata": {
        "id": "wUJoZmK-rHUA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "058ed0cc-fea0-4b0a-9a24-353989967528"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "خانه کردم‌ازيدی که درغان غم از يک عمر\n",
            "که ره قصر نيست که اين ناک فکب کندر ما را\n",
            "\n",
            "به ره اور که که مهش از ق\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.complete_text(text ='دوش',n_chars=700))"
      ],
      "metadata": {
        "id": "5ySP1eHVrJsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "دوش لطف عشق می‌رود\n",
        "\n",
        "ای اميدم ز قراراتم در آن خشت و خوش\n",
        "\n",
        "من خاک سرو گفت که در جان دردی بازم\n",
        "\n",
        "دوش ماه چشم گر غمی ز صد وفا آوردم\n",
        "\n",
        "\n",
        "غزل    ۳۶۱\n",
        "\n",
        "آتش دلبر يک دو صد هر روی ره نگارم\n",
        "\n",
        "چو صباح می‌بينی و کفن عارض است\n",
        "\n",
        "همنم فيضی توست نيست پاکدايم\n",
        "\n",
        "چال عرش نه اين حريفان نبرد هم\n",
        "\n",
        "گر به شوق زان عذوه‌ای ز کار هم\n",
        "\n",
        "آن کو می‌نام جوی روحی چه کنم\n",
        "\n",
        "اليست چرا مذهب را که هم\n",
        "\n",
        "روز عالم نام و طلب ما هست\n",
        "مرا\n",
        "\n",
        "عاشقان آمده‌ای روحی تو نس\n",
        "\n",
        "يدادن چه عذر ناتوشم بينی بنشينم\n",
        "\n",
        "\n",
        "غزل    ۳۵۷\n",
        "\n",
        "روزگاری و مرا چون پرور عهد بشود آن ماه\n",
        "\n",
        "همرايت که با دلدام جان و علم زدنيست\n",
        "\n",
        "طربسار برخيزيم و کان در نظر ليل\n",
        "\n",
        "حافظ از ياد روان با آمده ما بسی تا می‌بينم\n",
        "\n",
        "هرگز و نظر باده از آن خاک گو می‌بينم\n",
        "\n",
        "با دل از پی آزادگان می‌کنم\n",
        "\n",
        "عرزاست که می‌گذرم هر دم به طوف"
      ],
      "metadata": {
        "id": "YQfzNIl2UnD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stateful_model =Statefull_RNN()\n",
        "stateful_model.prepare_data('hafez.txt')\n",
        "stateful_model.build_layers()"
      ],
      "metadata": {
        "id": "cdvrD9OKrekj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7a070b5-08bb-49e4-83ca-8f7aa2652200"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stateful_model.train(epochs=1)"
      ],
      "metadata": {
        "id": "A8yBiFSErhDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stateful_model.complete_text('ساقی'))"
      ],
      "metadata": {
        "id": "39RLfrZVrmtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ساقی آن بهر حايت\n",
        "\n",
        "لغال  ع۲۶۲۰\n",
        "\n",
        "تا صب گران ايتوا سرحاسن قا ته خاسان\n",
        "\n",
        "سخظ حافظتمم لوی مارش کنيگوش\n",
        "\n",
        "انگ چه عاهر منيش تو بهته ما حرتان\n",
        "\n",
        "بخانيای شاقی خوار مخر ميوی\n",
        "\n",
        "صببه که می‌شدد و خل\n",
        "\n",
        "و دمد دود شاشافيا مض\n",
        "ديار"
      ],
      "metadata": {
        "id": "0SVHoPUIVOa0"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}